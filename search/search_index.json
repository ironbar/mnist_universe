{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MNIST Universe On this project I will take one of the most famous and simple datasets in the world, the MNIST dataset and I will think about how current artificial intelligence works, what are the differences with natural intelligence and how can we improve it. The name MNIST Universe comes from the fact that a model that is trained on the MNIST dataset only has access to a tiny fraction of the world. Its universe its limited to the 28x28 pixels of the images of the dataset.","title":"MNIST Universe"},{"location":"#mnist-universe","text":"On this project I will take one of the most famous and simple datasets in the world, the MNIST dataset and I will think about how current artificial intelligence works, what are the differences with natural intelligence and how can we improve it. The name MNIST Universe comes from the fact that a model that is trained on the MNIST dataset only has access to a tiny fraction of the world. Its universe its limited to the 28x28 pixels of the images of the dataset.","title":"MNIST Universe"},{"location":"00_mnist_dataset/","text":"MNIST Dataset I will make a short introduction to the MNIST dataset. Feel free to skip it if you are already familiar with it. MNIST is a dataset of images of handwritten digits. The images have a size of 28x28 and there are 60k images in the training set and 10k images in the test set. The images are grayscale and the values are between 0 and 255. The labels are integers between 0 and 9. The following image shows some examples of the images in the dataset. This dataset is very popular for tutorials because with the current computing power we can train a model that is very accurate in a short amount of time. The dataset is also very small so it is easy to download and use. Reference https://en.wikipedia.org/wiki/MNIST_database http://yann.lecun.com/exdb/mnist/","title":"MNIST Dataset"},{"location":"00_mnist_dataset/#mnist-dataset","text":"I will make a short introduction to the MNIST dataset. Feel free to skip it if you are already familiar with it. MNIST is a dataset of images of handwritten digits. The images have a size of 28x28 and there are 60k images in the training set and 10k images in the test set. The images are grayscale and the values are between 0 and 255. The labels are integers between 0 and 9. The following image shows some examples of the images in the dataset. This dataset is very popular for tutorials because with the current computing power we can train a model that is very accurate in a short amount of time. The dataset is also very small so it is easy to download and use.","title":"MNIST Dataset"},{"location":"00_mnist_dataset/#reference","text":"https://en.wikipedia.org/wiki/MNIST_database http://yann.lecun.com/exdb/mnist/","title":"Reference"},{"location":"01_typical_training/","text":"Typical training Let's see how a typical training looks like. I will be using Keras for this example. I have prepared a Collab notebook with all the code. The whole notebook runs in a few minutes. We just need numpy and keras for this experiment. import numpy as np from tensorflow import keras from tensorflow.keras import layers The MNIST dataset is loaded and preprocessed. def load_data(): # Model / data parameters num_classes = 10 input_shape = (28, 28, 1) # the data, split between train and test sets (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data() # Scale images to the [0, 1] range x_train = x_train.astype(\"float32\") / 255 x_test = x_test.astype(\"float32\") / 255 # Make sure images have shape (28, 28, 1) x_train = np.expand_dims(x_train, -1) x_test = np.expand_dims(x_test, -1) print(\"x_train shape:\", x_train.shape) print(x_train.shape[0], \"train samples\") print(x_test.shape[0], \"test samples\") # convert class vectors to binary class matrices y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) return x_train, y_train, (x_test, y_test) We define a very simple convolutional neural network architecture. def get_sample_model(input_shape=(28, 28, 1), num_classes=10): model = keras.Sequential( [ keras.Input(shape=input_shape), layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"), layers.MaxPooling2D(pool_size=(2, 2)), layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"), layers.MaxPooling2D(pool_size=(2, 2)), layers.Flatten(), layers.Dropout(0.5), layers.Dense(num_classes, activation=\"softmax\"), ] ) return model Finally we have the train function. def train_model_on_mnist(model, fit_kwargs=None, callbacks=None, compile_kwargs=None): default_compile_kwargs = dict(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]) if compile_kwargs is not None: default_compile_kwargs.update(compile_kwargs) model.compile(**default_compile_kwargs) default_callbacks = [ keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=10000), ] if callbacks is not None: default_callbacks += callbacks default_fit_kwargs = dict(verbose=0, epochs=1000, validation_data=test_data) if fit_kwargs is not None: default_fit_kwargs.update(fit_kwargs) ret = model.fit(x_train, y_train, callbacks=default_callbacks, **default_fit_kwargs) return ret.history And we call the train function to fit the model. x_train, y_train, test_data = load_data() train_model_on_mnist( get_sample_model(), fit_kwargs=dict(batch_size=128, verbose=1, epochs=100), compile_kwargs=dict(optimizer=keras.optimizers.Adam(1e-3)), callbacks=[ keras.callbacks.TensorBoard(log_dir='logs'), keras.callbacks.EarlyStopping(patience=15),] ); We can use Tensorboard to visualize the train (orange) and validation (blue) metrics during the training. Validation accuracy reaches 99.4% accuracy which is consistent with the state of the art. We have trained a neural network on the MNIST dataset, now let's think deeply about all the steps and its implications.","title":"Typical training"},{"location":"01_typical_training/#typical-training","text":"Let's see how a typical training looks like. I will be using Keras for this example. I have prepared a Collab notebook with all the code. The whole notebook runs in a few minutes. We just need numpy and keras for this experiment. import numpy as np from tensorflow import keras from tensorflow.keras import layers The MNIST dataset is loaded and preprocessed. def load_data(): # Model / data parameters num_classes = 10 input_shape = (28, 28, 1) # the data, split between train and test sets (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data() # Scale images to the [0, 1] range x_train = x_train.astype(\"float32\") / 255 x_test = x_test.astype(\"float32\") / 255 # Make sure images have shape (28, 28, 1) x_train = np.expand_dims(x_train, -1) x_test = np.expand_dims(x_test, -1) print(\"x_train shape:\", x_train.shape) print(x_train.shape[0], \"train samples\") print(x_test.shape[0], \"test samples\") # convert class vectors to binary class matrices y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) return x_train, y_train, (x_test, y_test) We define a very simple convolutional neural network architecture. def get_sample_model(input_shape=(28, 28, 1), num_classes=10): model = keras.Sequential( [ keras.Input(shape=input_shape), layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"), layers.MaxPooling2D(pool_size=(2, 2)), layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"), layers.MaxPooling2D(pool_size=(2, 2)), layers.Flatten(), layers.Dropout(0.5), layers.Dense(num_classes, activation=\"softmax\"), ] ) return model Finally we have the train function. def train_model_on_mnist(model, fit_kwargs=None, callbacks=None, compile_kwargs=None): default_compile_kwargs = dict(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]) if compile_kwargs is not None: default_compile_kwargs.update(compile_kwargs) model.compile(**default_compile_kwargs) default_callbacks = [ keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=10000), ] if callbacks is not None: default_callbacks += callbacks default_fit_kwargs = dict(verbose=0, epochs=1000, validation_data=test_data) if fit_kwargs is not None: default_fit_kwargs.update(fit_kwargs) ret = model.fit(x_train, y_train, callbacks=default_callbacks, **default_fit_kwargs) return ret.history And we call the train function to fit the model. x_train, y_train, test_data = load_data() train_model_on_mnist( get_sample_model(), fit_kwargs=dict(batch_size=128, verbose=1, epochs=100), compile_kwargs=dict(optimizer=keras.optimizers.Adam(1e-3)), callbacks=[ keras.callbacks.TensorBoard(log_dir='logs'), keras.callbacks.EarlyStopping(patience=15),] ); We can use Tensorboard to visualize the train (orange) and validation (blue) metrics during the training. Validation accuracy reaches 99.4% accuracy which is consistent with the state of the art. We have trained a neural network on the MNIST dataset, now let's think deeply about all the steps and its implications.","title":"Typical training"},{"location":"02_data/","text":"Data The model sees pairs of images and labels during training. For example it receives an image of a handwritten 0 and a label indicating that it is a 0. Thus the model only has access to a very limited subset of the universe. It does not have access to letters, animals, sounds... The dataset has many particular characteristics that will influence how well the model generalizes to new data. For example the images are grayscale, the numbers are centered... Also the model will not experience any notion of time . When a human is learning the numbers there is a time continuity on the senses inputs. If I see some numbers on a book those numbers won't change. I will be able to see them from different perspectives, distances and orientations. By contrary the inputs to the model change randomly: there is no relation between the image the model receives at one step and the next. In general a dataset is a biased and partial subset of the real world. A model trained on a dataset will learn the particularities of the dataset and because of that the performance of the model will be lower when being evaluated on a different dataset (with different biases). Train, validation and test data It is very common on machine learnig to partition the data on three disjoint subsets that are called train, validation and test. train : this is the data that the model will use to \"learn\", to fit its parameters. We can evaluate the model on this train data but this evaluation will have a very weak relation to the performance of the model with new data. Neural network models can have an arbitrary capacity and are able to memorize the train set and achieve a perfect score. validation : while training or at the end of the train we will evaluate our model on the validation data. This evaluation will be a more faithfull representation of how the model works with new data. We can use the validation score to guide the optimization of the hyperparameters of the model. However since we are doing multiple evaluations on the validation set there is information flowing to the model and we could not trust the validation score after many evaluations. test : when we have optimized the model using the validation scores then we could use the test to have a real estimation of how the model will work with new data. This process has some similarities to how a student prepares for a exam. The training set would be the books, video and other learning resources. The validation set would be exams from previous years. The real exam of that year would be the test set. If the student does a previous years' exam more than one time the score obtained on the following evaluations won't be a good estimator of the score on the real exam. Real data is noisy In an ideal world all the samples in a dataset will have correct labels. That is hardly the case in the real world. It is likely that even the famous MNIST dataset has wrong labels 1 : for example an image of a one labelled as a seven. This can happen both for human errors or for ambiguity in the definition of the problem. If the handwriting is not clear we could misclassify a seven by a one, or a nine by a four. A model with enough capacity trained on noisy data could achieve perfect accuracy by memorizing the errors. This makes the learning process harder because there are contradictions in the dataset. In those cases taking time to clean the dataset will usually pay off. There is a data-centric movement championed by Andrew Ng that places the focus on the data instead of the model. Even the MNIST dataset, assumed to be error-free and benchmarked in tens of thousands of peer-reviewed ML publications, contains 15 (human-validated) label errors in the test set. https://l7.curtisnorthcutt.com/label-errors \u21a9","title":"Data"},{"location":"02_data/#data","text":"The model sees pairs of images and labels during training. For example it receives an image of a handwritten 0 and a label indicating that it is a 0. Thus the model only has access to a very limited subset of the universe. It does not have access to letters, animals, sounds... The dataset has many particular characteristics that will influence how well the model generalizes to new data. For example the images are grayscale, the numbers are centered... Also the model will not experience any notion of time . When a human is learning the numbers there is a time continuity on the senses inputs. If I see some numbers on a book those numbers won't change. I will be able to see them from different perspectives, distances and orientations. By contrary the inputs to the model change randomly: there is no relation between the image the model receives at one step and the next. In general a dataset is a biased and partial subset of the real world. A model trained on a dataset will learn the particularities of the dataset and because of that the performance of the model will be lower when being evaluated on a different dataset (with different biases).","title":"Data"},{"location":"02_data/#train-validation-and-test-data","text":"It is very common on machine learnig to partition the data on three disjoint subsets that are called train, validation and test. train : this is the data that the model will use to \"learn\", to fit its parameters. We can evaluate the model on this train data but this evaluation will have a very weak relation to the performance of the model with new data. Neural network models can have an arbitrary capacity and are able to memorize the train set and achieve a perfect score. validation : while training or at the end of the train we will evaluate our model on the validation data. This evaluation will be a more faithfull representation of how the model works with new data. We can use the validation score to guide the optimization of the hyperparameters of the model. However since we are doing multiple evaluations on the validation set there is information flowing to the model and we could not trust the validation score after many evaluations. test : when we have optimized the model using the validation scores then we could use the test to have a real estimation of how the model will work with new data. This process has some similarities to how a student prepares for a exam. The training set would be the books, video and other learning resources. The validation set would be exams from previous years. The real exam of that year would be the test set. If the student does a previous years' exam more than one time the score obtained on the following evaluations won't be a good estimator of the score on the real exam.","title":"Train, validation and test data"},{"location":"02_data/#real-data-is-noisy","text":"In an ideal world all the samples in a dataset will have correct labels. That is hardly the case in the real world. It is likely that even the famous MNIST dataset has wrong labels 1 : for example an image of a one labelled as a seven. This can happen both for human errors or for ambiguity in the definition of the problem. If the handwriting is not clear we could misclassify a seven by a one, or a nine by a four. A model with enough capacity trained on noisy data could achieve perfect accuracy by memorizing the errors. This makes the learning process harder because there are contradictions in the dataset. In those cases taking time to clean the dataset will usually pay off. There is a data-centric movement championed by Andrew Ng that places the focus on the data instead of the model. Even the MNIST dataset, assumed to be error-free and benchmarked in tens of thousands of peer-reviewed ML publications, contains 15 (human-validated) label errors in the test set. https://l7.curtisnorthcutt.com/label-errors \u21a9","title":"Real data is noisy"},{"location":"03_model/","text":"Model An artificial neural network is just a parametric mathematical function. It receives some numbers as input and returns other numbers as output depending on the parameters of the function, which are typically called weights. In the case for the MNIST dataset the model will receive as input the value of the pixels of the images, and it will output a 10d vector representing the probability of the image being any of the digits from 0 to 9. The neural networks are typically structure in layers. Each layer has its own set of operations and parameters. Thus a neural network is a mathematical function that is created by composing smaller functions that we called layers. y = layer2(layer1(x)) The space of all the possible neural networks is infinite. The choice of the type of layers, numbers and connectivity between the layers is called the model architecture. Through trial and error the AI community has found certain architectures that work well for some specific problems. For example there are architectures that work well for image classification, others work well for time-series prediction... (Although there is a recent trend where it seems that the Transformer architecture is taking over all the applications) When we choose an architecture we are imposing some biases into the model. Those biases limit the capacity and expressivity of the model but if carefully chosen they will help the model to generalize better. For example convolutional layers apply the same filter over the whole image on a sliding window fashion. This bias implies that the position on a image won't be relevant to classify that object which makes sense. In theory if we had an infinite stream of data we will let an unbiased model to learn the structure of the data without constraints. We will do this because bias can guide the learning process in a limited set of data, but they also impose limits on the model performance (because there is almost always an exception to each rule)","title":"Model"},{"location":"03_model/#model","text":"An artificial neural network is just a parametric mathematical function. It receives some numbers as input and returns other numbers as output depending on the parameters of the function, which are typically called weights. In the case for the MNIST dataset the model will receive as input the value of the pixels of the images, and it will output a 10d vector representing the probability of the image being any of the digits from 0 to 9. The neural networks are typically structure in layers. Each layer has its own set of operations and parameters. Thus a neural network is a mathematical function that is created by composing smaller functions that we called layers. y = layer2(layer1(x)) The space of all the possible neural networks is infinite. The choice of the type of layers, numbers and connectivity between the layers is called the model architecture. Through trial and error the AI community has found certain architectures that work well for some specific problems. For example there are architectures that work well for image classification, others work well for time-series prediction... (Although there is a recent trend where it seems that the Transformer architecture is taking over all the applications) When we choose an architecture we are imposing some biases into the model. Those biases limit the capacity and expressivity of the model but if carefully chosen they will help the model to generalize better. For example convolutional layers apply the same filter over the whole image on a sliding window fashion. This bias implies that the position on a image won't be relevant to classify that object which makes sense. In theory if we had an infinite stream of data we will let an unbiased model to learn the structure of the data without constraints. We will do this because bias can guide the learning process in a limited set of data, but they also impose limits on the model performance (because there is almost always an exception to each rule)","title":"Model"},{"location":"04_training/","text":"Training Artificial neural networks are typically trained using the gradient descent algorithm. This algorithm modifies the model iteratively to minimize a loss function by computing the gradient of the loss function and propagating it through the neural network. The neural network and the loss function need to be differentiable if we want to use the gradient descent algorithm. On the MNIST dataset we might be interested in maximizing the accuracy of the model. However the accuracy function is not differentiable. So we have to use a proxy loss function called cross-entropy loss with the hope that minimizing the cross-entropy loss will result in an increase of the model accuracy. This process is no different from function optimization. So what makes Deep Learning special? Deep Learning scales well with model size and dataset size. We can get better models just by training a bigger model on a bigger dataset. Deep Learning imposes some constraints in the model: the model should have multiple sequential layers. This imposes a bias towards hierarchical processing Deep Learning has benefited from the increase of computing power and the increase in dataset sizes How similar is this to human learning? When humans are born they already have some innate capabilities. For example newborns have walking reflexes. In the other hand artificial neural networks are completely random at initialization, they have some inductive bias built in but no knowledge. When a baby grows it learns in a completely unsupervised way. It learns to crawl, walk and talk just by trial and error and imitation. When a child learns to talk and understands language it starts to receive a much informative reward signal from his parents. We as humans experience time as it passes and we have memories. The typical NN do not have any notion of time nor memories. A child can interact with the world, this allows to test hypothesis and learn causality. However it seems that from data alone is not possible to infer causality 1 . https://stats.stackexchange.com/questions/384330/is-causal-inference-only-from-data-possible \u21a9","title":"Training"},{"location":"04_training/#training","text":"Artificial neural networks are typically trained using the gradient descent algorithm. This algorithm modifies the model iteratively to minimize a loss function by computing the gradient of the loss function and propagating it through the neural network. The neural network and the loss function need to be differentiable if we want to use the gradient descent algorithm. On the MNIST dataset we might be interested in maximizing the accuracy of the model. However the accuracy function is not differentiable. So we have to use a proxy loss function called cross-entropy loss with the hope that minimizing the cross-entropy loss will result in an increase of the model accuracy. This process is no different from function optimization. So what makes Deep Learning special? Deep Learning scales well with model size and dataset size. We can get better models just by training a bigger model on a bigger dataset. Deep Learning imposes some constraints in the model: the model should have multiple sequential layers. This imposes a bias towards hierarchical processing Deep Learning has benefited from the increase of computing power and the increase in dataset sizes","title":"Training"},{"location":"04_training/#how-similar-is-this-to-human-learning","text":"When humans are born they already have some innate capabilities. For example newborns have walking reflexes. In the other hand artificial neural networks are completely random at initialization, they have some inductive bias built in but no knowledge. When a baby grows it learns in a completely unsupervised way. It learns to crawl, walk and talk just by trial and error and imitation. When a child learns to talk and understands language it starts to receive a much informative reward signal from his parents. We as humans experience time as it passes and we have memories. The typical NN do not have any notion of time nor memories. A child can interact with the world, this allows to test hypothesis and learn causality. However it seems that from data alone is not possible to infer causality 1 . https://stats.stackexchange.com/questions/384330/is-causal-inference-only-from-data-possible \u21a9","title":"How similar is this to human learning?"},{"location":"05_artificial_intelligence/","text":"Artificial Intelligence We have seen how a typical AI training looks like. We already have a trained model: is this model intelligent? That model is performing some operation (classifying handwritten digits) that we do with our brains. Thus the model is emulating a tiny process of our brain and we could say that it is somehow intelligent. But in the other hand the model is just a mathematical function. It can only do a very specialized task. On the Measure of Intelligence The intelligence of a system is a measure of its skill-acquisition efficiency over a scope of tasks, with respect to priors, experience, and generalization difficulty. Chollet takes away the focus on skill (how good a model is on Go, chess, image classification\u2026) and puts the focus on skill-acquisition efficiency. It says that if a task is fixed it it possible to buy arbitrary levels of skill given unlimited priors or unlimited training data. It argues that training on more data does not make the system more intelligent. Adding ever more data to a local-generalization learning system is certainly a fair strategy if one\u2019s end goal is skill on the task considered, but it will not lead to generalization beyond the data the system has seen This seems true in the case of MNIST. If we extend the dataset by searching for more images for each category is very likely that we are going to improve the skill of the system but it won't be more intelligent. However I'm not sure if this holds for Large Language Models. They are trained with a very general objetive: to predict the next word. It seems that to solve that task the model needs to develop many abilities. In the Palm paper they say that if they scale the model and the data they see emerging capabilities in the model. Thus according to the definition of Chollet a model trained on MNIST dataset is no intelligent. It does not have any adaptation capacity and it only knows to recognize digits. When presented with any other task it will completely fail. Generalization Let's think if the model will generalize to new data. Color data The model will likely work well with color images if they are previously transformed to gray scale. However it will likely fail when using images with white background (contrary to the black background seen during training) We should use data augmentation if we expect to find inverted images on production. The model is only going to work well on data that has similar properties to the train set. New symbols It is very likely that the model will missclassify new symbols that were not present on the train dataset.","title":"Artificial Intelligence"},{"location":"05_artificial_intelligence/#artificial-intelligence","text":"We have seen how a typical AI training looks like. We already have a trained model: is this model intelligent? That model is performing some operation (classifying handwritten digits) that we do with our brains. Thus the model is emulating a tiny process of our brain and we could say that it is somehow intelligent. But in the other hand the model is just a mathematical function. It can only do a very specialized task.","title":"Artificial Intelligence"},{"location":"05_artificial_intelligence/#on-the-measure-of-intelligence","text":"The intelligence of a system is a measure of its skill-acquisition efficiency over a scope of tasks, with respect to priors, experience, and generalization difficulty. Chollet takes away the focus on skill (how good a model is on Go, chess, image classification\u2026) and puts the focus on skill-acquisition efficiency. It says that if a task is fixed it it possible to buy arbitrary levels of skill given unlimited priors or unlimited training data. It argues that training on more data does not make the system more intelligent. Adding ever more data to a local-generalization learning system is certainly a fair strategy if one\u2019s end goal is skill on the task considered, but it will not lead to generalization beyond the data the system has seen This seems true in the case of MNIST. If we extend the dataset by searching for more images for each category is very likely that we are going to improve the skill of the system but it won't be more intelligent. However I'm not sure if this holds for Large Language Models. They are trained with a very general objetive: to predict the next word. It seems that to solve that task the model needs to develop many abilities. In the Palm paper they say that if they scale the model and the data they see emerging capabilities in the model. Thus according to the definition of Chollet a model trained on MNIST dataset is no intelligent. It does not have any adaptation capacity and it only knows to recognize digits. When presented with any other task it will completely fail.","title":"On the Measure of Intelligence"},{"location":"05_artificial_intelligence/#generalization","text":"Let's think if the model will generalize to new data.","title":"Generalization"},{"location":"05_artificial_intelligence/#color-data","text":"The model will likely work well with color images if they are previously transformed to gray scale. However it will likely fail when using images with white background (contrary to the black background seen during training) We should use data augmentation if we expect to find inverted images on production. The model is only going to work well on data that has similar properties to the train set.","title":"Color data"},{"location":"05_artificial_intelligence/#new-symbols","text":"It is very likely that the model will missclassify new symbols that were not present on the train dataset.","title":"New symbols"},{"location":"06_ml_techniques/","text":"Machine Learning techniques Regularization Regularization techniques impose some inductive bias to the models. We apply some priors with the believe that they will guide the model to find a simpler solution that will generalize better L1 regularization induces sparsity in the model, drives some of the weights to be zero L2 regularization favours smaller weights over big weights Dropout forces the model to be able to work with dead neurons. This favours the learning of robust features. Early stopping When using early stopping we will stop the training if the validation loss does not improve. It's a simple way to avoid overfitting to the train set.","title":"Machine Learning techniques"},{"location":"06_ml_techniques/#machine-learning-techniques","text":"","title":"Machine Learning techniques"},{"location":"06_ml_techniques/#regularization","text":"Regularization techniques impose some inductive bias to the models. We apply some priors with the believe that they will guide the model to find a simpler solution that will generalize better L1 regularization induces sparsity in the model, drives some of the weights to be zero L2 regularization favours smaller weights over big weights Dropout forces the model to be able to work with dead neurons. This favours the learning of robust features.","title":"Regularization"},{"location":"06_ml_techniques/#early-stopping","text":"When using early stopping we will stop the training if the validation loss does not improve. It's a simple way to avoid overfitting to the train set.","title":"Early stopping"},{"location":"07_emergence/","text":"Emergence unlimited priors or unlimited training data allow experimenters to \u201cbuy\u201d arbitrary levels of skills for a system, in a way that masks the system\u2019s own generalization power. Adding ever more data to a local-generalization learning system is certainly a fair strategy if one\u2019s end goal is skill on the task considered, but it will not lead to generalization beyond the data the system has seen Let's think it this holds for Large Language Models (LLM) and World Models. In the case of LLM a simple task like predicting the next word can force the model to learn facts about the world and to do non trivial tasks. For example the LLM can do language translation, question answering, mathematics... Emergent Abilities of Large Language Models","title":"Emergence"},{"location":"07_emergence/#emergence","text":"unlimited priors or unlimited training data allow experimenters to \u201cbuy\u201d arbitrary levels of skills for a system, in a way that masks the system\u2019s own generalization power. Adding ever more data to a local-generalization learning system is certainly a fair strategy if one\u2019s end goal is skill on the task considered, but it will not lead to generalization beyond the data the system has seen Let's think it this holds for Large Language Models (LLM) and World Models. In the case of LLM a simple task like predicting the next word can force the model to learn facts about the world and to do non trivial tasks. For example the LLM can do language translation, question answering, mathematics... Emergent Abilities of Large Language Models","title":"Emergence"},{"location":"08_causality/","text":"Causality The Book of Why The goal is to relate that to current NN models that do not interact with the world. An agent can observe the result of its actions in the world and learn simple causality. It can also observe the results of actions of other agents. 1. The Ladder of causation Ladder of causation First, very early in our evolution, we humans realized that the world is not made up only of dry facts(what we might call data today); rather these facts are glue together by an intricate web of cause-effect relationships. Second, causal explanations, not dry facts, make up the bulk of our knowledge, and should be the cornerstone of machine intelligence. The goal of strong AI is to produce machines with humanlike intelligence, able to converse with and guide humans. Deep Learning has instead given us machines with truly impressive abilities but no intelligence. The difference is profound and lies in the absence of a model of reality. The lack of flexibility and adaptability is inevitable in any system that works at the first level of the ladder of causation A sufficiently strong and accurate causal model can allow us to use rung-one (observational) data to answer rung-two (interventional) queries Cognitive revolution The Lion Man is a 40k year old sculpture. Is different from previous art because is a creature of previous imagination. Within 10k years after the Lion Man's creation all other hominids had become extinct. Humans evolved from apelike ancestors over a period of 6 million years. But in roughly the last 50k years something unique happened, which some call the Cognitive Revolution. Humans acquired the ability to modify their environment and their own abilities at a dramatically faster rate. Their newly acquired causal imagination enabled them to do many things more efficiently through a tricky process called \"planning\". Probability Decades' worth of experience with these kinds of questions has convinced me that, in both a cognitive and a philosophical sense, the idea of causes and effects is much more fundamental than the idea of probability. We begin learning causes and effects before we understand language and before we know any mathematics. Probabilities, as given by expressions like P(Y|X) , lie on the first rung of the Ladder of Causation and cannot ever (by themselves) answer queries on the second or third rung. While probabilities encode our beliefs about a static world, causality tells us whether and how probabilities change when the world changes, be it by intervention or by act of imagination. 2. From Buccaneers to Guinea Pigs This chapter tells the history of how the dominant school of thought in statistics banned causality from science and instead advocated for correlation. It's interesting to see the influence of eminent statisticians. It also tells how Wright battled with them with his path analysis technique. Regression to the mean Many events are influenced by luck. Success is one of them, thus when events are repeated is unlikely to achieve and extraordinary good result. History of Pearson and his preference for correlation over causation Guinea pigs You cannot draw causal conclusions without some causal hypotheses.This echoes what we concluded in Chapter 1: you cannot answer a question on rung two of the ladder of causation using only data collected from rung one. 3. From Evidence to Causes: Reverend Bayes meets Mr Holmes Bayes theorem Bayer theorem shows that you can deduce the probability of a cause from an effect. If we know the cause it is easy to estimate the probability of the effect, which is a forward probability. Going the other direction is harder. For example if we have covid then we know the probability of having a positive result in a covid test (simply look at the FNR of the test). However computing the probability of having covid given that we know that the result of the test is positive is not straightforward. We can estimate the conditional probability directly in one direction, for which our judgment is more reliable, and use mathematics to derive the conditional probability in the other direction, for which our judgment is rather hazy. P(C|T) P(T) = P(T|C) P(C) P(C|T) = \\frac{P(T|C) P(C)}{P(T)} = \\frac{P(T|C)}{P(T)} P(C) P(C|T) = \\frac{(1-FNR) P(C)}{(1-FNR) P(C) + FPR P(\\neg C)} With that equation we can compute the probability of having covid given that the result of the test was positive. There is an interesting example of mammogram test to check for breast cancer. Bayes theorem allows to update our prior belief about something using new information. The likelihood ratio modifies the initial belief. P(C|T) = \\frac{P(T|C)}{P(T)} P(C) likelihood ratio = \\frac{P(T|C)}{P(T)} Then there is a tiny glimpse at bayesian networks I felt convinced that any artificial intelligence would have to model itself on what we know about human neural information processing and that machine reasoning under uncertainty would have to be constructed with a similar message passing architecture. But what are the messages? This took me quite a few months to figure out. I finally realize that the messages were conditional probabilities in one direction and likelihood ratios in the other. Bayesian networks A bayesian network carries no assumption that the arrow has a causal meaning. The arrow merely signifies that we know the \"forward\" probability. Baye's rule tells us how to reverse the procedure, specifically by multiplying the prior probability by a likelihood ratio. Bayesian network on wikipedia Pattern Model Chain A\\rightarrow B\\rightarrow C Fork A\\leftarrow B\\rightarrow C Collider A\\rightarrow B\\leftarrow C Chain junction . In science one often thinks of B as the mechanism, or mediator that transmits the effect of A to C. A familiar example is Fire \\rightarrow Smoke \\rightarrow Alarm Fork junction . B is often called a confounder of A and C. The confounder will make A and C statistically correlated even thought there is no direct causal link between them. A good example is Shoe Size \\leftarrow Age of child \\rightarrow Reading ability We can eliminate this spurious correlation by conditioning on the child's age. If we look only at seven-years-olds we expect to see no relationship between shoe size and reading ability Collider junction . Talent \\rightarrow Celebrity \\leftarrow Beauty. Here we are asserting that both talent and beauty contribute to an actor's success, but beauty and talent are completely unrelated to one another in the general population. In this case if we condition on B we will see a negative correlation between A and B. If we look only at famous actors finding out that a celebrity is unattractive increases our belief that he is talented. The diagram describes the relation of the variables in a qualitative way, but if ou want quantitative answers you also need quantitative inputs. By depicting A as a root noted we do not really mean that A has no prior causes. We really mean that any prior causes of A can be adequately summarized in the prior probability. A bayesian network is integrative, which means that it reacts as a whole to any new information From Bayesian networks to causal diagrams Bayesian networks hold the key that enables causal diagrams to interface with data. All the probabilistic properties and the belief propagation algorithms remain valid in causal diagrams. In a causal diagram the direction of the arrows implies causality whereas on the bayesian networks it only implies the forward probability. Causal assumptions cannot be invented at our whim, they are subject to the scrutiny of data and can be falsified (probed that they are wrong) However the graphical properties of the diagram dictate which causal models can be distinguished by data and which will forever remain indistinguishable, no matter how large the data. For example we cannot distinguish the fork A\\leftarrow B\\rightarrow C from the chain A\\rightarrow B\\rightarrow C by data alone because with C listening to B only, the two imply the same independence conditions. This is very interesting because it says that data is not enough to create a causal diagram. We need to make hypothesis about the causal model. Whereas a Bayesian network can only tell us how likely one event is given that we observed another, causal diagrams can answer interventional and counterfactual questions. 4. Confounding and deconfounding: Or, Slaying the Lurking Variable We compare a group of people who get the treatment with a group of similar people who don't. Confounding bias occurs when a variable influences both who is selected for the treatment and the outcome of the experiment If the confounding variable Z is age, we compare the treatment and control groups in every age group separately. We can then take an average of the effects, weighting each age group according to its percentage in the target population. This method of compensation is familiar to all staticians; it is called \"adjusting for Z\" or \"controlling for Z\" Although confounding has a long history in all areas of science, the recognition that the problem requires causal, not statistical, solutions is very recent. Randomized controlled trial A randomized controlled trial (or randomized control trial; RCT) is a form of scientific experiment used to control factors not under direct experimental control. Participants who enroll in RCTs differ from one another in known and unknown ways that can influence study outcomes, and yet cannot be directly controlled. By randomly allocating participants among compared treatments, an RCT enables statistical control over these influences. Provided it is designed well, conducted properly, and enrolls enough participants, an RCT may achieve sufficient control over these confounding factors to deliver a useful comparison of the treatments studied. The one circumstance under which scientist will abandon some of their resistance to talk about causality is when they have conducted a randomized controlled trial Around 1923 Fisher began to realize that the only experimental design that the nature genie could not defeat was a random one Fisher realized that an uncertain answer to the right question is much better than a highly certain answer to the wrong question Randomization brings two benefits: First it eliminates the confounder bias (it asks Nature the right question). Second it enables the researcher to quantify his uncertainty. If you know what all the confounders are you can measure and adjust for them. However with randomization has the advantage of eliminating unknown confounders or confounders that we cannot control All things being equal RTC are still preferred to observational studies, just as safety nets are recommended for tightrope walkers. But in some cases intervention may not be possible. Confounding Confounding should be simply defined as anything that leads to a discrepancy between the two P(Y|X) \\neq P(Y|do(X)) Back-door criterion To deconfound two variables X and Y, we only need to to block every noncausal path between them without blocking or perturbing the causal paths. More precisely a back-door path is any path from X to Y that starts with an arrow pointing into X. X and Y will be deconfounded if we block every back-door path. If we do this by controlling for some set of variables Z, we also need to make sure that no member of Z is a descendant of X on a causal path. This are the rules to stop the flow of information throught any individual junction: In a chain junction A\\rightarrow B\\rightarrow C controlling for B prevents information about A from getting to C or viceversa In a fork junction A\\leftarrow B\\rightarrow C controlling for B prevents information about A from getting to C or viceversa In a collider A\\rightarrow B\\leftarrow C the variables A and C start out independent, but if you control for B information starts flowing. Controlling for descendants of a variable is like partially controlling for the variable itself I consider the complete solution of the confounding problem one of the main highlights of the causal revolution 5. The smoke-filled debate: clearing the air The problem of verifying that smoking caused cancer one of the most important scientific arguments against the smoking-cancer hypothesis was the possible existence of unmeasured factors that cause both craving for nicotine and lung cancer. some of the greatest milestones in medical history dealt with identifying causative agents. In the mid-1700s, James Lind had discovered that citrus fruits could prevent scurvy, and in the mid-1800s, John Snow had figured out that water contaminated with fecal matter caused cholera. These brilliant pieces of detective work had in common a fortunate one-to-one relation between cause and effect. The smoking-cancer debate challenged this monolithic concept of causation. Many people smoke their whole lives and never get lung cancer. Conversely, some people get lung cancer without ever lighting up a cigarette. statisticians already knew of one excellent way to establish causation in a more general sense: the randomized controlled trial (RCT). But such a study would be neither feasible nor ethical in the case of smoking. How they arrived to a conclusion He propose to compare patients who had already been diagnosed with cancer to a control group of healthy volunteers. The type of study Doll and Hill conducted is now called a case-control study because it compares \u201ccases\u201d (people with a disease) to controls. It is clearly an improvement over time series data, because researchers can control for confounders like age, sex, and exposure to environmental pollutants. This case-control design has the drawback of being retrospective: it tell us the probability that a cancer patient is a smoker instead of the probability that a smoker will get cancer. Instead of drawing inferences by assuming the absence of certain causal relationships in the model, the analyst challenges such assumptions and evaluates how strong alternative relationships must be in order to explain the observed data. This technique is called today \"sensitivity analysis\" To judge or evaluate the causal significance of the association between the attribute or agent and the disease, or effect upon health, a number of criteria must be utilized, no one of which is an all-sufficient basis for judgment.\u201d The committee listed five such criteria: consistency (many studies, in different populations, show similar results); strength of association (including the dose-response effect: more smoking is associated with a higher risk); specificity of the association (a particular agent should have a particular effect and not a long litany of effects); temporal relationship (the effect should follow the cause); and coherence (biological plausibility and consistency with other types of evidence such as laboratory experiments and time series). Viewed from the perspective of causality, the report was at best a modest success. It clearly established the gravity of causal questions and that data alone could not answer them. But as a roadmap for future discovery, its guidelines were uncertain and flimsy. Cornfield\u2019s inequality, which planted the seeds of sensitivity analysis, was a step in that direction. Paradox of smoking on newborns By looking only at babies with low birth weight, we are conditioning on that collider. This opens up a back-door path between Smoking and Mortality that goes Smoking \u2192 Birth Weight \u2190 Birth Defect \u2192 Mortality. This path is noncausal because one of the arrows goes the wrong way. 6. Paradoxes galore! This chapter is a collection of different paradoxes. It makes emphasis in the fact that data generation or selection process can cause correlations to appear: Monty Hall problem Simpson paradox Berkson's paradox Lord's paradox 7. Beyond Adjustment: The Conquest of Mount Intervention In this chapter we finally make our bold ascent onto the second level of the Ladder of Causation, the level of intervention\u2014the holy grail of causal thinking from antiquity to the present day. This level is involved in the struggle to predict the effects of actions and policies that haven\u2019t been tried yet, ranging from medical treatments to social programs, from economic policies to personal choices. Back door adjustment formula Linear approximation Regression coefficients For many researchers, the most (perhaps only) familiar method of predicting the effect of an intervention is to \u201ccontrol\u201d for confounders using the adjustment formula. This is the method to use if you are confident that you have data on a sufficient set of variables (called deconfounders) to block all the back-door paths between the intervention and the outcome. To do this, we measure the average causal effect of an intervention by first estimating its effect at each \u201clevel,\u201d or stratum, of the deconfounder. We then compute a weighted average of those strata, where each stratum is weighted according to its prevalence in the population. If, for example, the deconfounder is gender, we first estimate the causal effect for males and females. Then we average the two, if the population is (as usual) half male and half female. In short, sometimes a regression coefficient represents a causal effect, and sometimes it does not\u2014and you can\u2019t rely on the data alone to tell you the difference. Two additional ingredients are required to endow rYX.Z with causal legitimacy. First, the path diagram should represent a plausible picture of reality, and second, the adjusted variable(s) Z should satisfy the back-door criterion. Keep in mind also that the regression-based adjustment works only for linear models, which involve a major modeling assumption. The back-door criterion tells us which sets of variables we can use to deconfound our data. The adjustment formula actually does the deconfounding. adjustment does not work at all if there is a back-door path we cannot block because we don\u2019t have the requisite data. Front door adjustment Mediating variables Instead of going in the back door, we can go in the front door! In this case, the front door is the direct causal path Smoking \u2192 Tar \u2192 Cancer, for which we do have data on all three variables. Intuitively, the reasoning is as follows. First, we can estimate the average causal effect of Smoking on Tar, because there is no unblocked back-door path from Smoking to Tar, Likewise, the diagram allows us to estimate the average causal effect of Tar on Cancer. To do this we can block the back-door path from Tar to Cancer, Tar \u2190 Smoking \u2190 Smoking Gene \u2192 Cancer, by adjusting for Smoking. Now we know the average increase in the likelihood of tar deposits due to smoking and the average increase of cancer due to tar deposits. We can combine these to obtain the average increase in cancer due to smoking. Anytime the causal effect of X on Y is confounded by one set of variables (C) and mediated by another (M) (see Figure 7.2), and, furthermore, the mediating variables are shielded from the effects of C, then you can estimate X\u2019s effect from observational data. Glynn and Kashin\u2019s results show why the front-door adjustment is such a powerful tool: it allows us to control for confounders that we cannot observe (like Motivation), including those that we can\u2019t even name. RCTs are considered the \u201cgold standard\u201d of causal effect estimation for exactly the same reason. Do-calculus 3 rules of do calculus In both the front- and back-door adjustment formulas, the ultimate goal is to calculate the effect of an intervention, P(Y | do(X)), in terms of data such as P(Y | X, A, B, Z, \u2026) that do not involve a do-operator. If we are completely successful at eliminating the do\u2019s, then we can use observational data to estimate the causal effect, allowing us to leap from rung one to rung two of the Ladder of Causation. our central question of when a model can replace an experiment, or when a \u201cdo\u201d quantity can be reduced to a \u201csee\u201d quantity. Inspired by the ancient Greek geometers, we want to reduce the problem to symbol manipulation and in this way wrest causality from Mount Olympus and make it available to the average researcher. Rule 1 says when we observe a variable W that is irrelevant to Y (possibly conditional on other variables Z), then the probability distribution of Y will not change. Rule 2. We know that if a set Z of variables blocks all back-door paths from X to Y, then conditional on Z, do(X) is equivalent to see(X). We can, therefore, write Rule 3 is quite simple: it essentially says that we can remove do(X) from P(Y | do(X)) in any case where there are no causal paths from X to Y. That is, if we do something that does not affect Y, then the probability distribution of Y will not change. Let's write the rules of do-calculus in mathematical format: P(Y | do(X), Z, W) = P(Y | do(X), Z) if W is irrelevant to Y . P(Y | do(X), Z) = P(Y | X, Z) if Z blocks all back-door paths from X to Y . P(Y | do(X)) = P(Y) if X does not affect Y . Both groups independently and simultaneously proved that Rules 1 to 3 suffice to get out of any do-labyrinth that has an exit. It tells us that if we cannot find a way to estimate P(Y | do(X)) from Rules 1 to 3, then a solution does not exist. In that case, we know that there is no alternative to conducting a randomized controlled trial. Tapestry of science Talks about the people that help develop the science of causality. Good and bad cholesterol Sometimes in RTC we also have to use do-calculus because people do not follow the treatment. 8. Conterfactuals: mining worlds that could have been From Thucydides and Abraham to Hume and Lewis Review of conterfactual through history. An earthquake that caused a tsunami told by Thucdydides, the destruction of Sodoma and Gomorra if there are not enough rightfull men. And then talks about the definition of causation using conterfactuals. Potential outcomes, structural equations, and the algorithmization of conterfactuals Again more histories about Fisher and the struggle between statistics and causation. Then explains the problem of estimating the salary of one person given the education and experience. The conclusion is that we need to use a causal model to estimate the effect. We can test a causal model to see if the independence relations hold. The virtue of seeing your assumptions In this chapter it explains another method, I don't see the value because it is very confusing compared to the simple diagrams. Conterfactuals and the law It talks about sufficient and necessary causes. For example in a firing squad either of the soldiers is sufficient to cause the prisoner's death, and neither (in itself) is necessary. Necessary causes, sufficient causes and climate change Fraction of attributable risk (FAR) to quantify the effect of climate change. The FAR requires us to know two numbers: p0, the probability of a heat wave like the 2003 heat wave before climate change, and p1 the probabililty after climate change. For examplme if the probability doubles, then we can say that half of the risk is due to climate change. A world of conterfactuals 9. Mediation: The search for a mechanism Scurvy: the wrong mediator It telss the story of scurvy and how it was discovered in 1800 that eating citrus fruit prevented scurvy. However the mediation mechanism was not understood and that caused that 100 years later some expeditions suffered from scurvy because they started to believe that the problem was related to bad meat. In the early 1900s it was discovered that vitamine C that was present on citrus fruit was the mediator that prevented scurvy. Nature versus nurture: the tragedy of Barbara Burks Tells the story of a woman that invented causal paths independently of Wright. In search of a language (the Berkeley admissions paradox) Another story about a study of discrimination in a university. I don't see the point of telling this story. Daisy, the kittens and indirect effects Course of Causal Inference I'm dissapointed with the \"Book of Why\", I believe it gives a lot of unnecessary information and when it comes to give the relevant information it is not clear and not enough emphasys. Moreover the notation is not well chosen and that makes understanding more difficult. https://www.bradyneal.com/causal-inference-course The course is free and has: Videos Slides Book 1. Introduction Very nice introduction to the topic 2. Potential outcomes It explains the Fundamental Problem of Causal Inference: it is impossible to observe all potential outcomes for a given individual. Then the rest of the chapter is devoted to study why the associational difference is not usually equal to the average treatment effect because of confounding. It explains under what assumptions and how we can estimate the average treatment effect. Summary Causal Inference is the study of understanding the cause-and-effect relationships in data. Building a Directed Acyclic Graph (DAG) that represents the causal model typically requires a combination of domain knowledge, theoretical understanding, and, to some extent, data-driven insights. A DAG cannot be fully derived from data alone, as the causal relationships between variables are not inherently present in observational data. There might be multiple alternative causal models that cannot be falseable from data. Causal mechanisms describe the processes or pathways through which an effect is produced or a cause leads to an effect. These mechanisms often involve intermediate variables, also known as mediators, that transmit the causal effect from the independent variable (cause) to the dependent variable (effect). The ladder of causation is a three-level hierarchy of reasoning, introduced by Judea Pearl in \"The Book of Why,\" that describes the stages of understanding causal relationships: Association (bottom rung): At this level, the focus is on observing correlations and patterns in the data. It involves detecting and measuring associations between variables using statistical methods, such as correlation coefficients, regressions, or contingency tables. However, association does not imply causation. Intervention (middle rung): This level goes beyond mere associations and seeks to understand the causal effects of interventions. It answers questions like \"What if we do something?\" or \"What happens if we change a variable?\" To understand causal effects, researchers often perform controlled experiments, like randomized controlled trials, or use observational data and methods like matching, instrumental variables, or difference-in-differences to estimate causal relationships. Counterfactuals (top rung): Counterfactual reasoning represents the highest level of causal understanding. It involves asking \"What if\" questions about alternative realities or hypothetical scenarios that did not occur but could have. Counterfactual reasoning allows us to compare the actual outcome under the observed intervention with the potential outcome under a different intervention. This level of reasoning enables us to make causal attributions, explain observed phenomena, and inform decision-making. I believe I have a good understanding of causality now. There is some math that I still don't understand but I believe I should put the focus on AI, and only if needed delve deeper into causality.","title":"Causality"},{"location":"08_causality/#causality","text":"","title":"Causality"},{"location":"08_causality/#the-book-of-why","text":"The goal is to relate that to current NN models that do not interact with the world. An agent can observe the result of its actions in the world and learn simple causality. It can also observe the results of actions of other agents.","title":"The Book of Why"},{"location":"08_causality/#1-the-ladder-of-causation","text":"","title":"1. The Ladder of causation"},{"location":"08_causality/#ladder-of-causation","text":"First, very early in our evolution, we humans realized that the world is not made up only of dry facts(what we might call data today); rather these facts are glue together by an intricate web of cause-effect relationships. Second, causal explanations, not dry facts, make up the bulk of our knowledge, and should be the cornerstone of machine intelligence. The goal of strong AI is to produce machines with humanlike intelligence, able to converse with and guide humans. Deep Learning has instead given us machines with truly impressive abilities but no intelligence. The difference is profound and lies in the absence of a model of reality. The lack of flexibility and adaptability is inevitable in any system that works at the first level of the ladder of causation A sufficiently strong and accurate causal model can allow us to use rung-one (observational) data to answer rung-two (interventional) queries","title":"Ladder of causation"},{"location":"08_causality/#cognitive-revolution","text":"The Lion Man is a 40k year old sculpture. Is different from previous art because is a creature of previous imagination. Within 10k years after the Lion Man's creation all other hominids had become extinct. Humans evolved from apelike ancestors over a period of 6 million years. But in roughly the last 50k years something unique happened, which some call the Cognitive Revolution. Humans acquired the ability to modify their environment and their own abilities at a dramatically faster rate. Their newly acquired causal imagination enabled them to do many things more efficiently through a tricky process called \"planning\".","title":"Cognitive revolution"},{"location":"08_causality/#probability","text":"Decades' worth of experience with these kinds of questions has convinced me that, in both a cognitive and a philosophical sense, the idea of causes and effects is much more fundamental than the idea of probability. We begin learning causes and effects before we understand language and before we know any mathematics. Probabilities, as given by expressions like P(Y|X) , lie on the first rung of the Ladder of Causation and cannot ever (by themselves) answer queries on the second or third rung. While probabilities encode our beliefs about a static world, causality tells us whether and how probabilities change when the world changes, be it by intervention or by act of imagination.","title":"Probability"},{"location":"08_causality/#2-from-buccaneers-to-guinea-pigs","text":"This chapter tells the history of how the dominant school of thought in statistics banned causality from science and instead advocated for correlation. It's interesting to see the influence of eminent statisticians. It also tells how Wright battled with them with his path analysis technique.","title":"2. From Buccaneers to Guinea Pigs"},{"location":"08_causality/#regression-to-the-mean","text":"Many events are influenced by luck. Success is one of them, thus when events are repeated is unlikely to achieve and extraordinary good result.","title":"Regression to the mean"},{"location":"08_causality/#history-of-pearson-and-his-preference-for-correlation-over-causation","text":"","title":"History of Pearson and his preference for correlation over causation"},{"location":"08_causality/#guinea-pigs","text":"You cannot draw causal conclusions without some causal hypotheses.This echoes what we concluded in Chapter 1: you cannot answer a question on rung two of the ladder of causation using only data collected from rung one.","title":"Guinea pigs"},{"location":"08_causality/#3-from-evidence-to-causes-reverend-bayes-meets-mr-holmes","text":"","title":"3. From Evidence to Causes: Reverend Bayes meets Mr Holmes"},{"location":"08_causality/#bayes-theorem","text":"Bayer theorem shows that you can deduce the probability of a cause from an effect. If we know the cause it is easy to estimate the probability of the effect, which is a forward probability. Going the other direction is harder. For example if we have covid then we know the probability of having a positive result in a covid test (simply look at the FNR of the test). However computing the probability of having covid given that we know that the result of the test is positive is not straightforward. We can estimate the conditional probability directly in one direction, for which our judgment is more reliable, and use mathematics to derive the conditional probability in the other direction, for which our judgment is rather hazy. P(C|T) P(T) = P(T|C) P(C) P(C|T) = \\frac{P(T|C) P(C)}{P(T)} = \\frac{P(T|C)}{P(T)} P(C) P(C|T) = \\frac{(1-FNR) P(C)}{(1-FNR) P(C) + FPR P(\\neg C)} With that equation we can compute the probability of having covid given that the result of the test was positive. There is an interesting example of mammogram test to check for breast cancer. Bayes theorem allows to update our prior belief about something using new information. The likelihood ratio modifies the initial belief. P(C|T) = \\frac{P(T|C)}{P(T)} P(C) likelihood ratio = \\frac{P(T|C)}{P(T)} Then there is a tiny glimpse at bayesian networks I felt convinced that any artificial intelligence would have to model itself on what we know about human neural information processing and that machine reasoning under uncertainty would have to be constructed with a similar message passing architecture. But what are the messages? This took me quite a few months to figure out. I finally realize that the messages were conditional probabilities in one direction and likelihood ratios in the other.","title":"Bayes theorem"},{"location":"08_causality/#bayesian-networks","text":"A bayesian network carries no assumption that the arrow has a causal meaning. The arrow merely signifies that we know the \"forward\" probability. Baye's rule tells us how to reverse the procedure, specifically by multiplying the prior probability by a likelihood ratio. Bayesian network on wikipedia Pattern Model Chain A\\rightarrow B\\rightarrow C Fork A\\leftarrow B\\rightarrow C Collider A\\rightarrow B\\leftarrow C Chain junction . In science one often thinks of B as the mechanism, or mediator that transmits the effect of A to C. A familiar example is Fire \\rightarrow Smoke \\rightarrow Alarm Fork junction . B is often called a confounder of A and C. The confounder will make A and C statistically correlated even thought there is no direct causal link between them. A good example is Shoe Size \\leftarrow Age of child \\rightarrow Reading ability We can eliminate this spurious correlation by conditioning on the child's age. If we look only at seven-years-olds we expect to see no relationship between shoe size and reading ability Collider junction . Talent \\rightarrow Celebrity \\leftarrow Beauty. Here we are asserting that both talent and beauty contribute to an actor's success, but beauty and talent are completely unrelated to one another in the general population. In this case if we condition on B we will see a negative correlation between A and B. If we look only at famous actors finding out that a celebrity is unattractive increases our belief that he is talented. The diagram describes the relation of the variables in a qualitative way, but if ou want quantitative answers you also need quantitative inputs. By depicting A as a root noted we do not really mean that A has no prior causes. We really mean that any prior causes of A can be adequately summarized in the prior probability. A bayesian network is integrative, which means that it reacts as a whole to any new information","title":"Bayesian networks"},{"location":"08_causality/#from-bayesian-networks-to-causal-diagrams","text":"Bayesian networks hold the key that enables causal diagrams to interface with data. All the probabilistic properties and the belief propagation algorithms remain valid in causal diagrams. In a causal diagram the direction of the arrows implies causality whereas on the bayesian networks it only implies the forward probability. Causal assumptions cannot be invented at our whim, they are subject to the scrutiny of data and can be falsified (probed that they are wrong) However the graphical properties of the diagram dictate which causal models can be distinguished by data and which will forever remain indistinguishable, no matter how large the data. For example we cannot distinguish the fork A\\leftarrow B\\rightarrow C from the chain A\\rightarrow B\\rightarrow C by data alone because with C listening to B only, the two imply the same independence conditions. This is very interesting because it says that data is not enough to create a causal diagram. We need to make hypothesis about the causal model. Whereas a Bayesian network can only tell us how likely one event is given that we observed another, causal diagrams can answer interventional and counterfactual questions.","title":"From Bayesian networks to causal diagrams"},{"location":"08_causality/#4-confounding-and-deconfounding-or-slaying-the-lurking-variable","text":"We compare a group of people who get the treatment with a group of similar people who don't. Confounding bias occurs when a variable influences both who is selected for the treatment and the outcome of the experiment If the confounding variable Z is age, we compare the treatment and control groups in every age group separately. We can then take an average of the effects, weighting each age group according to its percentage in the target population. This method of compensation is familiar to all staticians; it is called \"adjusting for Z\" or \"controlling for Z\" Although confounding has a long history in all areas of science, the recognition that the problem requires causal, not statistical, solutions is very recent.","title":"4. Confounding and deconfounding: Or, Slaying the Lurking Variable"},{"location":"08_causality/#randomized-controlled-trial","text":"A randomized controlled trial (or randomized control trial; RCT) is a form of scientific experiment used to control factors not under direct experimental control. Participants who enroll in RCTs differ from one another in known and unknown ways that can influence study outcomes, and yet cannot be directly controlled. By randomly allocating participants among compared treatments, an RCT enables statistical control over these influences. Provided it is designed well, conducted properly, and enrolls enough participants, an RCT may achieve sufficient control over these confounding factors to deliver a useful comparison of the treatments studied. The one circumstance under which scientist will abandon some of their resistance to talk about causality is when they have conducted a randomized controlled trial Around 1923 Fisher began to realize that the only experimental design that the nature genie could not defeat was a random one Fisher realized that an uncertain answer to the right question is much better than a highly certain answer to the wrong question Randomization brings two benefits: First it eliminates the confounder bias (it asks Nature the right question). Second it enables the researcher to quantify his uncertainty. If you know what all the confounders are you can measure and adjust for them. However with randomization has the advantage of eliminating unknown confounders or confounders that we cannot control All things being equal RTC are still preferred to observational studies, just as safety nets are recommended for tightrope walkers. But in some cases intervention may not be possible.","title":"Randomized controlled trial"},{"location":"08_causality/#confounding","text":"Confounding should be simply defined as anything that leads to a discrepancy between the two P(Y|X) \\neq P(Y|do(X))","title":"Confounding"},{"location":"08_causality/#back-door-criterion","text":"To deconfound two variables X and Y, we only need to to block every noncausal path between them without blocking or perturbing the causal paths. More precisely a back-door path is any path from X to Y that starts with an arrow pointing into X. X and Y will be deconfounded if we block every back-door path. If we do this by controlling for some set of variables Z, we also need to make sure that no member of Z is a descendant of X on a causal path. This are the rules to stop the flow of information throught any individual junction: In a chain junction A\\rightarrow B\\rightarrow C controlling for B prevents information about A from getting to C or viceversa In a fork junction A\\leftarrow B\\rightarrow C controlling for B prevents information about A from getting to C or viceversa In a collider A\\rightarrow B\\leftarrow C the variables A and C start out independent, but if you control for B information starts flowing. Controlling for descendants of a variable is like partially controlling for the variable itself I consider the complete solution of the confounding problem one of the main highlights of the causal revolution","title":"Back-door criterion"},{"location":"08_causality/#5-the-smoke-filled-debate-clearing-the-air","text":"","title":"5. The smoke-filled debate: clearing the air"},{"location":"08_causality/#the-problem-of-verifying-that-smoking-caused-cancer","text":"one of the most important scientific arguments against the smoking-cancer hypothesis was the possible existence of unmeasured factors that cause both craving for nicotine and lung cancer. some of the greatest milestones in medical history dealt with identifying causative agents. In the mid-1700s, James Lind had discovered that citrus fruits could prevent scurvy, and in the mid-1800s, John Snow had figured out that water contaminated with fecal matter caused cholera. These brilliant pieces of detective work had in common a fortunate one-to-one relation between cause and effect. The smoking-cancer debate challenged this monolithic concept of causation. Many people smoke their whole lives and never get lung cancer. Conversely, some people get lung cancer without ever lighting up a cigarette. statisticians already knew of one excellent way to establish causation in a more general sense: the randomized controlled trial (RCT). But such a study would be neither feasible nor ethical in the case of smoking.","title":"The problem of verifying that smoking caused cancer"},{"location":"08_causality/#how-they-arrived-to-a-conclusion","text":"He propose to compare patients who had already been diagnosed with cancer to a control group of healthy volunteers. The type of study Doll and Hill conducted is now called a case-control study because it compares \u201ccases\u201d (people with a disease) to controls. It is clearly an improvement over time series data, because researchers can control for confounders like age, sex, and exposure to environmental pollutants. This case-control design has the drawback of being retrospective: it tell us the probability that a cancer patient is a smoker instead of the probability that a smoker will get cancer. Instead of drawing inferences by assuming the absence of certain causal relationships in the model, the analyst challenges such assumptions and evaluates how strong alternative relationships must be in order to explain the observed data. This technique is called today \"sensitivity analysis\" To judge or evaluate the causal significance of the association between the attribute or agent and the disease, or effect upon health, a number of criteria must be utilized, no one of which is an all-sufficient basis for judgment.\u201d The committee listed five such criteria: consistency (many studies, in different populations, show similar results); strength of association (including the dose-response effect: more smoking is associated with a higher risk); specificity of the association (a particular agent should have a particular effect and not a long litany of effects); temporal relationship (the effect should follow the cause); and coherence (biological plausibility and consistency with other types of evidence such as laboratory experiments and time series). Viewed from the perspective of causality, the report was at best a modest success. It clearly established the gravity of causal questions and that data alone could not answer them. But as a roadmap for future discovery, its guidelines were uncertain and flimsy. Cornfield\u2019s inequality, which planted the seeds of sensitivity analysis, was a step in that direction.","title":"How they arrived to a conclusion"},{"location":"08_causality/#paradox-of-smoking-on-newborns","text":"By looking only at babies with low birth weight, we are conditioning on that collider. This opens up a back-door path between Smoking and Mortality that goes Smoking \u2192 Birth Weight \u2190 Birth Defect \u2192 Mortality. This path is noncausal because one of the arrows goes the wrong way.","title":"Paradox of smoking on newborns"},{"location":"08_causality/#6-paradoxes-galore","text":"This chapter is a collection of different paradoxes. It makes emphasis in the fact that data generation or selection process can cause correlations to appear: Monty Hall problem Simpson paradox Berkson's paradox Lord's paradox","title":"6. Paradoxes galore!"},{"location":"08_causality/#7-beyond-adjustment-the-conquest-of-mount-intervention","text":"In this chapter we finally make our bold ascent onto the second level of the Ladder of Causation, the level of intervention\u2014the holy grail of causal thinking from antiquity to the present day. This level is involved in the struggle to predict the effects of actions and policies that haven\u2019t been tried yet, ranging from medical treatments to social programs, from economic policies to personal choices.","title":"7. Beyond Adjustment: The Conquest of Mount Intervention"},{"location":"08_causality/#back-door-adjustment-formula","text":"Linear approximation Regression coefficients For many researchers, the most (perhaps only) familiar method of predicting the effect of an intervention is to \u201ccontrol\u201d for confounders using the adjustment formula. This is the method to use if you are confident that you have data on a sufficient set of variables (called deconfounders) to block all the back-door paths between the intervention and the outcome. To do this, we measure the average causal effect of an intervention by first estimating its effect at each \u201clevel,\u201d or stratum, of the deconfounder. We then compute a weighted average of those strata, where each stratum is weighted according to its prevalence in the population. If, for example, the deconfounder is gender, we first estimate the causal effect for males and females. Then we average the two, if the population is (as usual) half male and half female. In short, sometimes a regression coefficient represents a causal effect, and sometimes it does not\u2014and you can\u2019t rely on the data alone to tell you the difference. Two additional ingredients are required to endow rYX.Z with causal legitimacy. First, the path diagram should represent a plausible picture of reality, and second, the adjusted variable(s) Z should satisfy the back-door criterion. Keep in mind also that the regression-based adjustment works only for linear models, which involve a major modeling assumption. The back-door criterion tells us which sets of variables we can use to deconfound our data. The adjustment formula actually does the deconfounding. adjustment does not work at all if there is a back-door path we cannot block because we don\u2019t have the requisite data.","title":"Back door adjustment formula"},{"location":"08_causality/#front-door-adjustment","text":"Mediating variables Instead of going in the back door, we can go in the front door! In this case, the front door is the direct causal path Smoking \u2192 Tar \u2192 Cancer, for which we do have data on all three variables. Intuitively, the reasoning is as follows. First, we can estimate the average causal effect of Smoking on Tar, because there is no unblocked back-door path from Smoking to Tar, Likewise, the diagram allows us to estimate the average causal effect of Tar on Cancer. To do this we can block the back-door path from Tar to Cancer, Tar \u2190 Smoking \u2190 Smoking Gene \u2192 Cancer, by adjusting for Smoking. Now we know the average increase in the likelihood of tar deposits due to smoking and the average increase of cancer due to tar deposits. We can combine these to obtain the average increase in cancer due to smoking. Anytime the causal effect of X on Y is confounded by one set of variables (C) and mediated by another (M) (see Figure 7.2), and, furthermore, the mediating variables are shielded from the effects of C, then you can estimate X\u2019s effect from observational data. Glynn and Kashin\u2019s results show why the front-door adjustment is such a powerful tool: it allows us to control for confounders that we cannot observe (like Motivation), including those that we can\u2019t even name. RCTs are considered the \u201cgold standard\u201d of causal effect estimation for exactly the same reason.","title":"Front door adjustment"},{"location":"08_causality/#do-calculus","text":"3 rules of do calculus In both the front- and back-door adjustment formulas, the ultimate goal is to calculate the effect of an intervention, P(Y | do(X)), in terms of data such as P(Y | X, A, B, Z, \u2026) that do not involve a do-operator. If we are completely successful at eliminating the do\u2019s, then we can use observational data to estimate the causal effect, allowing us to leap from rung one to rung two of the Ladder of Causation. our central question of when a model can replace an experiment, or when a \u201cdo\u201d quantity can be reduced to a \u201csee\u201d quantity. Inspired by the ancient Greek geometers, we want to reduce the problem to symbol manipulation and in this way wrest causality from Mount Olympus and make it available to the average researcher. Rule 1 says when we observe a variable W that is irrelevant to Y (possibly conditional on other variables Z), then the probability distribution of Y will not change. Rule 2. We know that if a set Z of variables blocks all back-door paths from X to Y, then conditional on Z, do(X) is equivalent to see(X). We can, therefore, write Rule 3 is quite simple: it essentially says that we can remove do(X) from P(Y | do(X)) in any case where there are no causal paths from X to Y. That is, if we do something that does not affect Y, then the probability distribution of Y will not change. Let's write the rules of do-calculus in mathematical format: P(Y | do(X), Z, W) = P(Y | do(X), Z) if W is irrelevant to Y . P(Y | do(X), Z) = P(Y | X, Z) if Z blocks all back-door paths from X to Y . P(Y | do(X)) = P(Y) if X does not affect Y . Both groups independently and simultaneously proved that Rules 1 to 3 suffice to get out of any do-labyrinth that has an exit. It tells us that if we cannot find a way to estimate P(Y | do(X)) from Rules 1 to 3, then a solution does not exist. In that case, we know that there is no alternative to conducting a randomized controlled trial.","title":"Do-calculus"},{"location":"08_causality/#tapestry-of-science","text":"Talks about the people that help develop the science of causality.","title":"Tapestry of science"},{"location":"08_causality/#good-and-bad-cholesterol","text":"Sometimes in RTC we also have to use do-calculus because people do not follow the treatment.","title":"Good and bad cholesterol"},{"location":"08_causality/#8-conterfactuals-mining-worlds-that-could-have-been","text":"","title":"8. Conterfactuals: mining worlds that could have been"},{"location":"08_causality/#from-thucydides-and-abraham-to-hume-and-lewis","text":"Review of conterfactual through history. An earthquake that caused a tsunami told by Thucdydides, the destruction of Sodoma and Gomorra if there are not enough rightfull men. And then talks about the definition of causation using conterfactuals.","title":"From Thucydides and Abraham to Hume and Lewis"},{"location":"08_causality/#potential-outcomes-structural-equations-and-the-algorithmization-of-conterfactuals","text":"Again more histories about Fisher and the struggle between statistics and causation. Then explains the problem of estimating the salary of one person given the education and experience. The conclusion is that we need to use a causal model to estimate the effect. We can test a causal model to see if the independence relations hold.","title":"Potential outcomes, structural equations, and the algorithmization of conterfactuals"},{"location":"08_causality/#the-virtue-of-seeing-your-assumptions","text":"In this chapter it explains another method, I don't see the value because it is very confusing compared to the simple diagrams.","title":"The virtue of seeing your assumptions"},{"location":"08_causality/#conterfactuals-and-the-law","text":"It talks about sufficient and necessary causes. For example in a firing squad either of the soldiers is sufficient to cause the prisoner's death, and neither (in itself) is necessary.","title":"Conterfactuals and the law"},{"location":"08_causality/#necessary-causes-sufficient-causes-and-climate-change","text":"Fraction of attributable risk (FAR) to quantify the effect of climate change. The FAR requires us to know two numbers: p0, the probability of a heat wave like the 2003 heat wave before climate change, and p1 the probabililty after climate change. For examplme if the probability doubles, then we can say that half of the risk is due to climate change.","title":"Necessary causes, sufficient causes and climate change"},{"location":"08_causality/#a-world-of-conterfactuals","text":"","title":"A world of conterfactuals"},{"location":"08_causality/#9-mediation-the-search-for-a-mechanism","text":"","title":"9. Mediation: The search for a mechanism"},{"location":"08_causality/#scurvy-the-wrong-mediator","text":"It telss the story of scurvy and how it was discovered in 1800 that eating citrus fruit prevented scurvy. However the mediation mechanism was not understood and that caused that 100 years later some expeditions suffered from scurvy because they started to believe that the problem was related to bad meat. In the early 1900s it was discovered that vitamine C that was present on citrus fruit was the mediator that prevented scurvy.","title":"Scurvy: the wrong mediator"},{"location":"08_causality/#nature-versus-nurture-the-tragedy-of-barbara-burks","text":"Tells the story of a woman that invented causal paths independently of Wright.","title":"Nature versus nurture: the tragedy of Barbara Burks"},{"location":"08_causality/#in-search-of-a-language-the-berkeley-admissions-paradox","text":"Another story about a study of discrimination in a university. I don't see the point of telling this story.","title":"In search of a language (the Berkeley admissions paradox)"},{"location":"08_causality/#daisy-the-kittens-and-indirect-effects","text":"","title":"Daisy, the kittens and indirect effects"},{"location":"08_causality/#course-of-causal-inference","text":"I'm dissapointed with the \"Book of Why\", I believe it gives a lot of unnecessary information and when it comes to give the relevant information it is not clear and not enough emphasys. Moreover the notation is not well chosen and that makes understanding more difficult. https://www.bradyneal.com/causal-inference-course The course is free and has: Videos Slides Book","title":"Course of Causal Inference"},{"location":"08_causality/#1-introduction","text":"Very nice introduction to the topic","title":"1. Introduction"},{"location":"08_causality/#2-potential-outcomes","text":"It explains the Fundamental Problem of Causal Inference: it is impossible to observe all potential outcomes for a given individual. Then the rest of the chapter is devoted to study why the associational difference is not usually equal to the average treatment effect because of confounding. It explains under what assumptions and how we can estimate the average treatment effect.","title":"2. Potential outcomes"},{"location":"08_causality/#summary","text":"Causal Inference is the study of understanding the cause-and-effect relationships in data. Building a Directed Acyclic Graph (DAG) that represents the causal model typically requires a combination of domain knowledge, theoretical understanding, and, to some extent, data-driven insights. A DAG cannot be fully derived from data alone, as the causal relationships between variables are not inherently present in observational data. There might be multiple alternative causal models that cannot be falseable from data. Causal mechanisms describe the processes or pathways through which an effect is produced or a cause leads to an effect. These mechanisms often involve intermediate variables, also known as mediators, that transmit the causal effect from the independent variable (cause) to the dependent variable (effect). The ladder of causation is a three-level hierarchy of reasoning, introduced by Judea Pearl in \"The Book of Why,\" that describes the stages of understanding causal relationships: Association (bottom rung): At this level, the focus is on observing correlations and patterns in the data. It involves detecting and measuring associations between variables using statistical methods, such as correlation coefficients, regressions, or contingency tables. However, association does not imply causation. Intervention (middle rung): This level goes beyond mere associations and seeks to understand the causal effects of interventions. It answers questions like \"What if we do something?\" or \"What happens if we change a variable?\" To understand causal effects, researchers often perform controlled experiments, like randomized controlled trials, or use observational data and methods like matching, instrumental variables, or difference-in-differences to estimate causal relationships. Counterfactuals (top rung): Counterfactual reasoning represents the highest level of causal understanding. It involves asking \"What if\" questions about alternative realities or hypothetical scenarios that did not occur but could have. Counterfactual reasoning allows us to compare the actual outcome under the observed intervention with the potential outcome under a different intervention. This level of reasoning enables us to make causal attributions, explain observed phenomena, and inform decision-making. I believe I have a good understanding of causality now. There is some math that I still don't understand but I believe I should put the focus on AI, and only if needed delve deeper into causality.","title":"Summary"}]}